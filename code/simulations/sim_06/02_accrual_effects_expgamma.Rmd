---
title: "Simulations to explore accrual rate influence on power/ppos"
subtitle: "`r knitr::current_input(dir = TRUE)`"
author: "Mark Jones"
date: "`r Sys.time()`"
output:
  html_document:
    number_sections: yes
    self_contained: yes
    theme: united
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 3
  word_document:
    toc: yes
    toc_depth: '3'
geometry: left=0.2cm,right=0.2cm,top=1cm,bottom=1cm
editor_options:
  chunk_output_type: console
---

<!--    toc: yes
    toc_float: true -->

<style type="text/css">
.main-container {
  max-width: 1800px;
  margin-left: auto;
  margin-right: auto;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.path = 'figs/')
suppressPackageStartupMessages(library(simstudy))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(mcmc))
suppressPackageStartupMessages(library(survival))
suppressPackageStartupMessages(library(doParallel))
suppressPackageStartupMessages(library(foreach))
suppressPackageStartupMessages(library(poisson))

ggplot2::theme_set(ggplot2::theme_bw())
ggplot2::theme_update(text = element_text(size = 10))
ggplot2::theme_update(legend.position = "top")
# ggplot2::theme_update(legend.title = element_blank())
ggplot2::theme_update(axis.text.x = element_text(size = 10))
ggplot2::theme_update(axis.text.y = element_text(size = 10))

# Work|Right|Fast
# rmarkdown::render("simulation_report.Rmd", clean=TRUE)

# logit to p
inv_logit <- function(x){
  return(exp(x)/(1+exp(x)))
}
# p to logit
logit <- function(p){
  return(log(p/(1-p)))
}
prob_to_odd <- function(x){
  return(x/(1-x))
}
odd_to_prob <- function(x){
  return(x/(1+x))
}

```

# Preamble

Assume some nominal target sample size at each interim, say 200, 300, 400, 500 and run a time to event analysis with subjects followed up to a maximum of 45 months.

# Generate Data

Exponential model. Control and treatment group. Parameters set so that the control median time to event is 30 months and the treatment arm median time to event is 37 months.

```{r}
trial_dat <- function(trialid, n, t0, delta, enro_rate = 3){
  t1 <- t0 + delta
  b0 <- log(2)/t0
  b1 <- log(2)/t1 - log(2)/t0

  d <- data.frame(trialid = trialid, id = 1:n)
  
  d$enro_t <- nhpp.sim(rate = enro_rate, 
                       num.events = n, 
                       prob.func = function(t) 1, 
                       prepend.t0 = F)
  
  d$trt <- rep(0:1, len = n)
  d$y <- rexp(n, b0 + b1*d$trt)
  d$c <- ifelse(d$y > cen_t, 1, 0)
  d$y <- ifelse(d$c == 1, cen_t, d$y)
  d
}

# 1000 trials each with interims def by interm_n
nsim <- 1000
# obs per trial
N <- 300 
# med tte
t0 <- 20
# trt effect
delta <- 7

cen_t <- 40
```

Gamma conjugate prior to exponential likelihood - parameterisation is based on rate. Sufficient statistics are number of events and total observed time.

```{r}
# Sufficient statistics based on those enrolled at the
# time of the interim.
suff <- function(dat, idx){
  
  # can only view a subset of the data
  c <- dat$c[1:idx]
  y <- dat$y[1:idx]
  trt <- dat$trt[1:idx]
  
  n_uncen_0 <- sum(c[trt == 0]==0)
  n_uncen_1 <- sum(c[trt == 1]==0)
  obst_0 <- sum(y[trt == 0])
  obst_1 <- sum(y[trt == 1])

  return(list(n_uncen_0 = n_uncen_0,
              n_uncen_1 = n_uncen_1,
              obst_0 = obst_0,
              obst_1 = obst_1,
              n = length(y)))
}
```

Interim analysis. At each interim extract the target number of subjects from the current trial data. The sufficient stats are computed on the enrolled subset. The posterior for the rate parameter is computed based on the observed data for the control and treatment arm. This posterior is then used to simulate data for all observations that are censored. Similarly we use the posterior to simulate the subjects that are not yet enrolled to make our current interim sample size up to the target size. All observations are then censored at 45 months so that we have the same follow up period for all individuals. The simulated data is then used to compute the posterior values for the rate parameter in each group. Finally, the empirical probability that the rate ratio is greater than 1 is computed. The process is repeated for x thousand particles.

```{r}

do_interim <- function(x=1, idx_intrm=1, accrual){
  
  n_post_draw <- 2000
  
  # this is the target accrual rate - it dictates how many will be imputed
  n_interim_target <- seq(200, 600, by = 100)
  
  # grab the precomputed data from subj 1 through to subject 
  # at the target sample size
  dat <- d[[x]][1:n_interim_target[idx_intrm],]

  # compute sufficient stats from those that are enrolled
  lsuff <- suff(dat, idx = accrual[idx_intrm])
  
  # sample from the posterior
  # use a gamma(1, 50) prior throughout - est mean = 0.02
  l0 <- rgamma(n_post_draw, 1 + lsuff$n_uncen_0, 50 + lsuff$obst_0)
  l1 <- rgamma(n_post_draw, 1 + lsuff$n_uncen_1, 50 + lsuff$obst_1)
  
  # save original state (event times and censoring)
  idx0 <- which(dat$trt == 0)
  idx1 <- which(dat$trt == 1)
  y0 <- dat$y[idx0]
  y1 <- dat$y[idx1]
  c0 <- dat$c[idx0]
  c1 <- dat$c[idx1]
  
  # prob of ratio of hazards being > 1
  ptx <- numeric(n_post_draw)

  # how many do we need to impute to get to the target size?
  n_impute <- length(seq(accrual[idx_intrm]+1, n_interim_target[idx_intrm], by = 1))
  
  for(i in 1:n_post_draw){
    
    # for the subset we need to impute, 
    # set the time to event to zero and censoring to 1
    dat$y[seq(accrual[idx_intrm]+1, n_interim_target[idx_intrm], by = 1)] <- 0
    dat$c[seq(accrual[idx_intrm]+1, n_interim_target[idx_intrm], by = 1)] <- 1
  
    # impute values for ALL censored (using memoryless property of expo)
    dat$y[idx0 & dat$c[idx0] == 1] <- dat$y[idx0 & dat$c[idx0] == 1] + 
      rexp(sum(idx0 & dat$c[idx0] == 1), l0[i])
    
    # as above for the trt group
    dat$y[idx1 & dat$c[idx1] == 1] <- dat$y[idx1 & dat$c[idx1] == 1] + 
      rexp(sum(idx1 & dat$c[idx1] == 1), l1[i])
    
    # everyone now has a time to event and now censor at 45 so 
    # that everyone has the same follow up.
    dat$c <- ifelse(dat$y > cen_t, 1, 0)
    dat$y <- ifelse(dat$c == 1, cen_t, dat$y)

    # do analysis
    lsuff <- suff(dat, idx = n_interim_target[idx_intrm])
    lpp0 <- rgamma(n_post_draw, 1 + lsuff$n_uncen_0, 50 + lsuff$obst_0)
    lpp1 <- rgamma(n_post_draw, 1 + lsuff$n_uncen_1, 50 + lsuff$obst_1)
    ptx[i] <- mean((lpp0/lpp1)>1)
    
    # now reset y and c to what they were originally 
    # so we are ready for the next round
    dat$y[idx0] <- y0
    dat$y[idx1] <- y1
    dat$c[idx0] <- c0
    dat$c[idx1] <- c1
  
  }

  ptx
}

```

Pre-generate data with slow accrual

```{r}
d <- lapply(1:nsim, trial_dat, N, t0, delta, enro_rate = 2)

head(d[[1]])
```


Simulate.

```{r}
cl <- makeCluster(parallel::detectCores() - 2, outfile="")
registerDoParallel(cl)
# registerDoSEQ()
results <- foreach(i = 1:nsim,
                   .errorhandling = 'pass'#,
                   # .packages=c()
                   #.export = c("do_interim", "suff", "d")
                   ) %dopar%{
    
  # assume accrual is going slow e.g. 40 behind target per interim
  n_interim_target <- seq(200, 600, by = 100)
  n_interims_test <- c(160, 220, 280, 340, 400)

  for(j in 1:length(n_interims_test)){
    
    ptx <- do_interim(i, j, accrual = n_interims_test)
    if(mean(ptx) > 0.70){
      return(list(i=i, j=n_interims_test[j], ptx = mean(ptx), es = 1))
    }

  }
  return(list(i=i, j=n_interims_test[j], ptx = mean(ptx), es = 0))
}
stopCluster(cl)

res <- do.call(rbind, lapply(1:nsim, function(x) unlist(results[[x]])))
rdsfname <- paste0("res-",format(Sys.time(), "%Y-%m-%d-%H-%M-%S"), ".RDS")
saveRDS(list(results=res), rdsfname)
summary(res)

```

Simulate with fast accrual.

```{r}
cl <- makeCluster(parallel::detectCores() - 2, outfile="")
registerDoParallel(cl)
# registerDoSEQ()
results <- foreach(i = 1:nsim,
                   .errorhandling = 'pass'#,
                   # .packages=c()
                   #.export = c("do_interim", "suff", "d")
                   ) %dopar%{
    
  # assume accrual is going slow e.g. 40 behind target per interim
  n_interim_target <- seq(200, 600, by = 100)
  n_interims_test <- c(190, 280, 370, 460, 550)

  for(j in 1:length(n_interims_test)){
    
    ptx <- do_interim(i, j, accrual = n_interims_test)
    if(mean(ptx) > 0.70){
      return(list(i=i, j=n_interims_test[j], ptx = mean(ptx), es = 1))
    }

  }
  return(list(i=i, j=n_interims_test[j], ptx = mean(ptx), es = 0))
}
stopCluster(cl)

res <- do.call(rbind, lapply(1:nsim, function(x) unlist(results[[x]])))
rdsfname <- paste0("res-",format(Sys.time(), "%Y-%m-%d-%H-%M-%S"), ".RDS")
saveRDS(list(results=res), rdsfname)
summary(res)

```



```{r, echo = F, eval = F}
# 
# 
# rdsfname <- paste0("res-",format(Sys.time(), "%Y-%m-%d-%H-%M-%S"), ".RDS")
# saveRDS(list(results=dfres1, interim_post = dfres2, cfg = cfg, warnings = w), rdsfname)
# 
#   n_interim_fast_accru <- c(290, 380, 470)
# 
# # particles 
# 
# # choose an interim
# # in theory power should decrease as accrual rates get higher...
# 
# res <- lapply(1:nsim, do_interim, accrual = n_interims_slow_accru)
# saveRDS(res, "res_slow.RDS")
# pr1 <- unlist(lapply(1:nsim, function(x)mean(res[[x]])))
# 
# res <- lapply(1:nsim, do_interim, accrual = n_interim_med_accru)
# saveRDS(res, "res_med.RDS")
# pr2 <- unlist(lapply(1:nsim, function(x)mean(res[[x]])))
# 
# res <- lapply(1:nsim, do_interim, accrual = n_interim_target_accru)
# saveRDS(res, "res_target.RDS")
# pr3 <- unlist(lapply(1:nsim, function(x)mean(res[[x]])))
# 
# 
# den1 <- density(pr1)
# den2 <- density(pr2)
# den3 <- density(pr3)
# 
# plot(den1, main = "Prob that hazd ratio > 1")
# lines(den2, lty = 2)
# lines(den3, lty = 3)
# legend("topleft",
#        legend = c("Slow accrual", "Med accrual", "Fast accrual"), 
#        lty = 1:3)
```

# Summary

This seems to suggest that as accrual increases the probability of detecting that the hazard ratio > 1 also increases. 





