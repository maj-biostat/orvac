---
title: "Simulations to explore accrual rate influence on power/ppos"
subtitle: "`r knitr::current_input(dir = TRUE)`"
author: "Mark Jones"
date: "`r Sys.time()`"
output:
  html_document:
    number_sections: yes
    self_contained: yes
    theme: united
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 3
  word_document:
    toc: yes
    toc_depth: '3'
geometry: left=0.2cm,right=0.2cm,top=1cm,bottom=1cm
editor_options:
  chunk_output_type: console
---

<!--    toc: yes
    toc_float: true -->

<style type="text/css">
.main-container {
  max-width: 1800px;
  margin-left: auto;
  margin-right: auto;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.path = 'figs/')
suppressPackageStartupMessages(library(simstudy))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(mcmc))
suppressPackageStartupMessages(library(survival))

ggplot2::theme_set(ggplot2::theme_bw())
ggplot2::theme_update(text = element_text(size = 10))
ggplot2::theme_update(legend.position = "top")
# ggplot2::theme_update(legend.title = element_blank())
ggplot2::theme_update(axis.text.x = element_text(size = 10))
ggplot2::theme_update(axis.text.y = element_text(size = 10))

# Work|Right|Fast
# rmarkdown::render("simulation_report.Rmd", clean=TRUE)

# logit to p
inv_logit <- function(x){
  return(exp(x)/(1+exp(x)))
}
# p to logit
logit <- function(p){
  return(log(p/(1-p)))
}
prob_to_odd <- function(x){
  return(x/(1-x))
}
odd_to_prob <- function(x){
  return(x/(1+x))
}

```

# Preamble

Assume you have interims starting at $n_{min}$ then every $n$ patients up to a maximum sample size $N$. If you vary $n$ per interim (the accrual rate) how does it effect power in a survival endpoint (exponential)? 

# Simulate interims

Generate datasets.

Exponential model. Censor at 45 months.

```{r}
trial_dat <- function(trialid, n, t0, delta){
  t1 <- t0 + delta
  b0 <- log(2)/t0
  b1 <- log(2)/t1 - log(2)/t0

  d <- data.frame(trialid = trialid, id = 1:n)
  # complete balance 1:1
  d$trt <- rep(0:1, len = n)
  d$y <- rexp(n, b0 + b1*d$trt)
  d$c <- 0
  d$c <- ifelse(d$y > 30, 1, 0)
  d$e <- 1 - d$c
  # assume they were 6 months when they were randomised
  # therefore once the time gets above 30, they will be
  # censored
  d$y <- ifelse(d$c == 1, 30, d$y)
  d
}

# 1000 trials each with interims def by interm_n
nsim <- 1000
# obs per trial
N <- 1000 
# baseline med tte
t0 <- 35
# trt effect (months increase)
delta <- 10
# censoring at 36 months of age (assuming rand at 6 months)
d <- lapply(1:nsim, trial_dat, N, t0, delta)

head(d[[1]])
```

Compute sample sizes when interim analyses happen assuming a given accrual rate.

```{r}
get_int <- function(n){
  nmin <- 200
  interm_n <- seq(nmin, N, by = n)
  if(max(interm_n)!=N){
    interm_n <- c(interm_n, N)
  } 
  interm_n
}
# e.g.
get_int(n = 30)
```

Gamma conjugate prior to exponential likelihood - parameterisation is based on rate.

```{r}
# Sufficient statistics based on those enrolled at the
# time of the interim.
suff <- function(dat, idx){
  
  # can only view a subset of the data
  c <- dat$c[1:idx]
  y <- dat$y[1:idx]
  trt <- dat$trt[1:idx]
  
  n_uncen_0 <- sum(c[trt == 0]==0)
  n_uncen_1 <- sum(c[trt == 1]==0)
  obst_0 <- sum(y[trt == 0])
  obst_1 <- sum(y[trt == 1])

  return(list(n_uncen_0 = n_uncen_0,
              n_uncen_1 = n_uncen_1,
              obst_0 = obst_0,
              obst_1 = obst_1,
              n = length(y)))
}


# turn interim into function simulate 1000 times.
do_interim <- function(x, accrual = n_interims_slow_accru){
  
  # grab the precomputed data from subj 1 through to subject 
  # at the target sample size
  dat <- d[[x]][1:n_interim_target[idx_intrm],]

  # compute sufficient stats from those that are enrolled
  lsuff <- suff(dat, idx = accrual[idx_intrm])
  
  # sample from the posterior
  l0 <- rgamma(n_post_draw, 1 + lsuff$n_uncen_0, 10 + lsuff$obst_0)
  l1 <- rgamma(n_post_draw, 1 + lsuff$n_uncen_1, 10 + lsuff$obst_1)
  
  # save original state (event times and censoring)
  idx0 <- which(dat$trt == 0)
  idx1 <- which(dat$trt == 1)
  y0 <- dat$y[idx0]
  y1 <- dat$y[idx1]
  c0 <- dat$c[idx0]
  c1 <- dat$c[idx1]
  
  # prob of ratio of hazards being > 1
  ptx <- numeric(n_post_draw)

  # how many do we need to impute to get to the target size?
  n_impute <- length(seq(accrual[idx_intrm]+1, n_target_interim[idx_intrm], by = 1))
  
  msg = 1
  if(msg == 1) {message(" Imputing ", n_impute); msg = 0}
  
  for(i in 1:n_post_draw){
    
    # for the subset we need to impute, 
    # set the time to event to zero and censoring to 1
    dat$y[seq(accrual[idx_intrm]+1, n_target_interim[idx_intrm], by = 1)] <- 0
    dat$c[seq(accrual[idx_intrm]+1, n_target_interim[idx_intrm], by = 1)] <- 1
  
    # impute values for ALL censored (using memoryless property of expo)
    dat$y[idx0 & dat$c[idx0] == 1] <- dat$y[idx0 & dat$c[idx0] == 1] + 
      rexp(sum(idx0 & dat$c[idx0] == 1), l0[i])
    
    # as above for the trt group
    dat$y[idx1 & dat$c[idx1] == 1] <- dat$y[idx1 & dat$c[idx1] == 1] + 
      rexp(sum(idx1 & dat$c[idx1] == 1), l1[i])
    
    # everyone now has a time to event.
    dat$c <- 0
    
    # do analysis
    lsuff <- suff(dat, idx = n_interim_target[idx_intrm])
    lpp0 <- rgamma(n_post_draw, 10 + lsuff$n_uncen_0, 0.1 + lsuff$obst_0)
    lpp1 <- rgamma(n_post_draw, 10 + lsuff$n_uncen_1, 0.1 + lsuff$obst_1)
    ptx[i] <- mean((lpp0/lpp1)>1)
    
    # now reset y and c to what they were originally 
    # so we are ready for the next round
    dat$y[idx0] <- y0
    dat$y[idx1] <- y1
    dat$c[idx0] <- c0
    dat$c[idx1] <- c1
  
  }

  ptx
}


# particles 
n_post_draw <- 1000
# choose an interim
# in theory power should decrease as accrual rates get higher...
idx_intrm <- 10
res <- lapply(1:nsim, do_interim, accrual = n_interims_slow_accru)
saveRDS(res, "res_slow.RDS")
pr1 <- unlist(lapply(1:nsim, function(x)mean(res[[x]])))

res <- lapply(1:nsim, do_interim, accrual = n_interim_med_accru)
saveRDS(res, "res_med.RDS")
pr2 <- unlist(lapply(1:nsim, function(x)mean(res[[x]])))

res <- lapply(1:nsim, do_interim, accrual = n_interim_target_accru)
saveRDS(res, "res_target.RDS")
pr3 <- unlist(lapply(1:nsim, function(x)mean(res[[x]])))


den1 <- density(pr1)
den2 <- density(pr2)
den3 <- density(pr3)

plot(den1, main = "Prob that hazd ratio > 1")
lines(den2, lty = 2)
lines(den3, lty = 3)
legend("topleft",
       legend = c("Slow accrual", "Med accrual", "Fast accrual"), 
       lty = 1:3)

fit <- function(current_n, sim_id){
  # retrieve the observations 1 to the size of the current 
  # interim from the relevant trial dataset and run a 
  # logrank test
  lm1 <- survdiff(Surv(y, 1-c) ~ trt, data=d[[sim_id]][1:current_n,])
  pchisq(lm1$chisq,df = 1, lower.tail = F)
}

sim_interims <- function(sim_id, n_step){
  
  interims <- get_int(n_step)
  
  # at each interim do the analysis
  res <- unlist(lapply(interims, fit, sim_id))
  # if a difference is detected in any of the analyses (adjusting for
  # multiplicity) then return true (implies expected success)
  any(p.adjust(res, method = "bonferroni") < 0.05)
}

```

Do simulations.

```{r}
# set up params of interest
accrual_rates <- c(30, 40, 50)
t0 <- c(20, 35, 50)
deltas <- c(5, 7.5, 10, 12.5, 15)
nsim <- 10000

dres <- expand.grid(deltas, t0, accrual_rates)
names(dres) <- c("delta", "t0", "accrual_per_q")
dres$ppos <- 0

for(i in 1:length(accrual_rates)){
  
  for(j in 1:length(t0)){
    
    for(k in 1:length(deltas)){
      
      d <- lapply(1:nsim, trial_dat, N, t0[j], deltas[k])
      
      ppos <- mean(unlist(lapply(1:nsim, sim_interims, n_step = accrual_rates[i])))
      
      dres[dres$delta == deltas[k] & 
             dres$t0 == t0[j] &
             dres$accrual_per_q == accrual_rates[i], "ppos"] <- ppos
      
    }

  }

}


```

Plot.

```{r, echo = F, fig.width=5, fig.height=5}
ggplot(dres, aes(x = delta, y = ppos, 
                 lty = paste0(accrual_per_q),
                 col = paste0(t0)))+
  geom_line() +
  scale_color_discrete("Baseline med tte")+
  scale_linetype_discrete("Accrual per interim")+
  theme(legend.direction = "horizontal") +
  theme(legend.position = "bottom") +
  theme(legend.box = "vertical")+
  scale_y_continuous("Pr(e.s)", lim = c(0, 1))+
  scale_x_continuous("Diff in med surv tim")
```

# Summary

This seems to suggest that as accrual increases the probability of detecting that the hazard ratio > 1 also increases. 





